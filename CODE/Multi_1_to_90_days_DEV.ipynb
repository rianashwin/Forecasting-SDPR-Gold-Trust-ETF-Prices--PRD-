{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPDR Gold Trust ETF Price Prediction - Modelling\n",
    "\n",
    "The purpose of this notebook is to explore modelling techniques before pushing to PRD.\n",
    "\n",
    "### Import packages, define functions, load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from math import sqrt\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# https://stackoverflow.com/questions/40516661/adding-line-to-scatter-plot-using-pythons-matplotlib\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Define functions\n",
    "#######################################################\n",
    "def remove_missing_targets(this_data,target_var):\n",
    "    '''\n",
    "    arg: target\n",
    "    outputs: raw data without rows where target is missing\n",
    "    '''\n",
    "    this_data = this_data[~this_data[target_var].isnull()]\n",
    "    this_data = this_data[~this_data[\"Date\"].isnull()]\n",
    "        \n",
    "    return this_data\n",
    "    \n",
    "\n",
    "def treat_missing_feature_values_adjusted(my_verbose, this_data):\n",
    "    '''\n",
    "    outputs: interpolates missing values\n",
    "    '''\n",
    "    cols_to_adj = [\"Swiss_Francs_Index\", \"EURO_Index\", \"Yen_Index\"]\n",
    "    this_data[cols_to_adj] = this_data[cols_to_adj].replace({0:np.nan})\n",
    "    this_data.loc[~(this_data['Crude_Oil_Futures'] > 0), 'Crude_Oil_Futures']=np.nan    \n",
    "    \n",
    "    this_data = this_data.interpolate(method='spline', order=1)\n",
    "    \n",
    "    if my_verbose==True:\n",
    "        print(\"\\nMissing values have been treated\")\n",
    "    \n",
    "    return this_data\n",
    "\n",
    "\n",
    "def detrend_data(this_data,my_verbose):\n",
    "    '''\n",
    "    Fits polynomials to trend lines and calculates difference\n",
    "    '''\n",
    "    dict_degrees = {\n",
    "        'SPDR_Gold_Shares': 2,\n",
    "        'Gold_Futures': 2,\n",
    "        'Crude_Oil_Futures': 4,\n",
    "        'Palladium_Futures': 3,\n",
    "        'Platinum_Futures': 4,\n",
    "        'Copper_Futures': 4,\n",
    "        'SP_500': 4,\n",
    "        'Russell_2000': 4,\n",
    "        'US_Dollar_Index': 4,\n",
    "        'Swiss_Francs_Index': 3,\n",
    "        'EURO_Index': 3,\n",
    "        'Yen_Index': 4\n",
    "    }\n",
    "\n",
    "    X_vals = this_data.index\n",
    "    X_vals = np.reshape(X_vals, (len(X_vals), 1))\n",
    "    detrended_data = pd.DataFrame()\n",
    "    detrended_data[\"Date\"] = this_data.Date.values\n",
    "    this_data = this_data[[x for x in this_data.columns if x!=\"Date\"]] # and x!=\"SPDR_Gold_Shares\"]]\n",
    "\n",
    "    for this_col in this_data.columns.tolist():\n",
    "        detrended_vals = []\n",
    "        if my_verbose!=False:\n",
    "            print(\"\\n\", this_col)\n",
    "        raw_feature = this_data[this_col].values.reshape(-1,1)\n",
    "        pf = PolynomialFeatures(degree=dict_degrees[this_col])\n",
    "        Xp = pf.fit_transform(X_vals)\n",
    "        md2 = LinearRegression()\n",
    "        md2.fit(Xp, raw_feature)\n",
    "        trendp = md2.predict(Xp)\n",
    "        \n",
    "        if my_verbose!=False:\n",
    "            plt.plot(X_vals, raw_feature)\n",
    "            plt.plot(X_vals, trendp)\n",
    "            plt.legend(['data', 'polynomial trend'])\n",
    "            plt.show()\n",
    "\n",
    "        detrended_data[\"{}_polynomial_transform\".format(this_col)] = trendp\n",
    "\n",
    "        detrpoly = [raw_feature[i] - trendp[i] for i in range(0, len(raw_feature))]\n",
    "        \n",
    "        if my_verbose!=False:\n",
    "            plt.plot(X_vals, detrpoly)\n",
    "            plt.title('polynomially detrended data')\n",
    "            plt.show()\n",
    "\n",
    "        r2 = r2_score(raw_feature, trendp)\n",
    "        rmse = np.sqrt(mean_squared_error(raw_feature, trendp))\n",
    "        \n",
    "        if my_verbose!=False:\n",
    "            print('r2:', r2)\n",
    "            print('rmse', rmse)\n",
    "\n",
    "        for i in detrpoly:\n",
    "            detrended_vals.append(i[0])\n",
    "        detrended_data[\"{}_detrended\".format(this_col)] = detrended_vals\n",
    "        \n",
    "    return detrended_data\n",
    "\n",
    "\n",
    "def remove_outliers(this_data,stdev_multiplier,my_verbose):\n",
    "    '''\n",
    "    Removes outliers which are beyond certain threshold based on st dev\n",
    "    '''\n",
    "    detrended_features = [x for x in this_data.columns.tolist() if x != \"Date\" and x.endswith(\"detrended\")]\n",
    "\n",
    "    for this_feature in detrended_features:\n",
    "        this_features_mean, this_features_stdev = np.mean(this_data[this_feature].values), np.std(this_data[this_feature].values)\n",
    "        this_features_lowerbound, this_features_upperbound = this_features_mean - (stdev_multiplier*this_features_stdev), this_features_mean + (stdev_multiplier*this_features_stdev)\n",
    "        this_data['{}_is_outlier'.format(this_feature)] = this_data.apply(lambda row: 'Is_Outlier' if row[this_feature]<this_features_lowerbound or  row[this_feature]>this_features_upperbound else 'Not_Outlier',axis=1)\n",
    "\n",
    "    this_data.reset_index(inplace=True)\n",
    "    \n",
    "    if my_verbose!=False:\n",
    "        for this_feature in detrended_features:\n",
    "            plt.figure()#figsize=(12,5))\n",
    "            sns.scatterplot(data=this_data, x=\"index\", y=this_feature, hue=\"{}_is_outlier\".format(this_feature))\n",
    "        \n",
    "    this_data = this_data.replace(\"Is_Outlier\",1)\n",
    "    this_data = this_data.replace(\"Not_Outlier\",0)\n",
    "    outlier_cols = [x for x in this_data.columns.tolist() if x.endswith(\"outlier\")]\n",
    "    this_data[\"Outlier_Indicator\"] = this_data[outlier_cols].sum(axis = 1, skipna = True)\n",
    "    this_data = this_data[this_data[\"Outlier_Indicator\"]==0]\n",
    "    cols_keep = [x for x in this_data.columns.tolist() if x not in outlier_cols and x!= \"index\" and x!=\"Outlier_Indicator\"]\n",
    "    this_data = this_data[cols_keep]\n",
    "    \n",
    "    return this_data\n",
    "\n",
    "\n",
    "def calculate_macd_and_spreadvssignal(my_verbose, this_data, cols_to_calculate):\n",
    "    '''\n",
    "    outputs: dataframe with macd and macd vs spread for each shifted_col\n",
    "    '''    \n",
    "    \n",
    "    for this_col in cols_to_calculate:\n",
    "\n",
    "        exp1 = this_data[this_col].ewm(span=12, adjust=False).mean()\n",
    "        exp2 = this_data[this_col].ewm(span=26, adjust=False).mean()\n",
    "        macd = exp1-exp2\n",
    "        signal = macd.ewm(span=9, adjust=False).mean()\n",
    "        macd_signal_spread = macd - signal\n",
    "\n",
    "        this_data[\"{}_macd\".format(this_col)] =  macd.values #.tolist()\n",
    "        this_data[\"{}_macd_signal_spread\".format(this_col)] =  macd_signal_spread.values #.tolist()\n",
    "    \n",
    "    if my_verbose==True:\n",
    "        print(\"\\nMACD and spread computed\")\n",
    "        \n",
    "    #### transformed_data.to_csv(r'MACD.csv')\n",
    "    \n",
    "    return this_data\n",
    "\n",
    "\n",
    "def calculate_moving_averages(my_verbose, this_data, cols_to_calculate):\n",
    "    '''\n",
    "    outputs: dataframe with spread of price vs SMA and EMA, with rows with missing SMA removed\n",
    "    '''    \n",
    "\n",
    "    for this_col in cols_to_calculate:\n",
    "        this_data['{}/15SMA'.format(this_col)] = (this_data[this_col]/(this_data[this_col].rolling(window=15).mean()))-1\n",
    "        this_data['{}/30SMA'.format(this_col)] = (this_data[this_col]/(this_data[this_col].rolling(window=30).mean()))-1\n",
    "        this_data['{}/60SMA'.format(this_col)] = (this_data[this_col]/(this_data[this_col].rolling(window=60).mean()))-1\n",
    "        this_data['{}/90SMA'.format(this_col)] = (this_data[this_col]/(this_data[this_col].rolling(window=90).mean()))-1\n",
    "        #this_data['{}/180SMA'.format(this_col)] = (this_data[this_col]/(this_data[this_col].rolling(window=180).mean()))-1\n",
    "\n",
    "\n",
    "\n",
    "    for this_col in cols_to_calculate:\n",
    "        this_data['{}/90EMA'.format(this_col)] = (this_data[this_col]/(this_data[this_col].ewm(span=90,adjust=True,ignore_na=True).mean()))-1\n",
    "        #this_data['{}/180EMA'.format(this_col)] = (this_data[this_col]/(this_data[this_col].ewm(span=180,adjust=True,ignore_na=True).mean()))-1    \n",
    "\n",
    "\n",
    "    this_data = this_data.iloc[179:] # take from row 181 onwards,otherwise SMA has null values\n",
    "    \n",
    "    if my_verbose==True:\n",
    "        print(\"\\nSpreads vs moving averages computed\")\n",
    "    \n",
    "    #### transformed_data.to_csv(r'SMA_EMA.csv')\n",
    "    \n",
    "    return this_data\n",
    "\n",
    "\n",
    "def scale_data(this_data, target_var):\n",
    "    '''\n",
    "    scales features and targets separately\n",
    "    '''        \n",
    "\n",
    "    selected_cols = [x for x in this_data.columns.tolist() if x!=\"Date\" and x!=target_var]\n",
    "\n",
    "    # fit on training & validation, transform training & validation, and true\n",
    "    this_training_and_validation_data = this_data.iloc[:-90]\n",
    "    this_true_data = this_data.iloc[-90:]\n",
    "\n",
    "    this_x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_this_training_and_validation_data_x = this_x_scaler.fit_transform(this_training_and_validation_data[selected_cols])\n",
    "    scaled_this_true_data_x = this_x_scaler.transform(this_true_data[selected_cols])\n",
    "    scaled_this_full_data_x = np.append(scaled_this_training_and_validation_data_x, scaled_this_true_data_x, axis=0)\n",
    "    scaled_data_x = pd.DataFrame(data=scaled_this_full_data_x,columns=selected_cols)\n",
    "    scaled_data_x.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    this_y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_this_training_and_validation_data_y = this_y_scaler.fit_transform(this_training_and_validation_data[target_var].values.reshape(-1, 1))\n",
    "    scaled_this_true_data_y = this_y_scaler.transform(this_true_data[target_var].values.reshape(-1, 1))\n",
    "    scaled_this_full_data_y = np.append(scaled_this_training_and_validation_data_y, scaled_this_true_data_y, axis=0)\n",
    "    scaled_data_y = pd.DataFrame(data=scaled_this_full_data_y,columns=[target_var])\n",
    "    scaled_data_y.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    scaled_data = pd.merge(scaled_data_x, scaled_data_y, left_index=True, right_index=True)\n",
    "\n",
    "    return scaled_data, this_y_scaler\n",
    "\n",
    "\n",
    "def inverse_scale_target(this_scaler,this_data,target_var):\n",
    "    '''\n",
    "    function to descale target\n",
    "    ''' \n",
    "    descaled_data = this_scaler.inverse_transform(this_data)\n",
    "\n",
    "    descaled_data = pd.DataFrame(data=descaled_data,columns=[target_var])\n",
    "    \n",
    "    return descaled_data\n",
    "\n",
    "\n",
    "def shift_values(my_verbose, this_data, forecasting_horizon, target_var):\n",
    "    '''\n",
    "    arg:\n",
    "    - forecasting horizon\n",
    "    - target_var. We still shift the target var so that we can calculate lagged macd and spread\n",
    "    '''    \n",
    "    this_data = this_data.reset_index()\n",
    "    this_data.drop(\"index\",axis=1,inplace=True)\n",
    "    df_appended = this_data.copy()\n",
    "    placeholder_shifted_data = this_data.copy()\n",
    "    cols_to_shift = [x for x in this_data.columns.tolist() if x!=\"Date\"]\n",
    "    \n",
    "    if my_verbose==True:\n",
    "        print(\"\\nShifting horizon:\", forecasting_horizon, \"\\n\")    \n",
    "\n",
    "    for this_col in cols_to_shift:\n",
    "        if my_verbose==True:\n",
    "            print(\"Shifting column:\", this_col)\n",
    "        lagged_feature = []\n",
    "        temp_df = this_data[[this_col]].copy()\n",
    "        temp_shifted = pd.DataFrame(temp_df[this_col].shift(+forecasting_horizon))\n",
    "        placeholder_shifted_data = placeholder_shifted_data.join(temp_shifted.rename(columns=lambda x: x + \"_shifted\"))\n",
    "        if this_col != target_var:\n",
    "            placeholder_shifted_data.drop(this_col,axis=1,inplace=True)\n",
    "        \n",
    "    placeholder_shifted_data = placeholder_shifted_data.iloc[forecasting_horizon:]\n",
    "    \n",
    "    return placeholder_shifted_data\n",
    "    \n",
    "    \n",
    "def calculate_corr(target_var,min_corr,this_data):    \n",
    "    '''\n",
    "    calculates correlation and returns features above a certain threshold\n",
    "    ''' \n",
    "    temp_df = pd.DataFrame(this_data.corr(method=\"spearman\")[np.abs(this_data.corr(method=\"spearman\"))>=min_corr].iloc[0,:].copy())\n",
    "    temp_df.sort_index(inplace=True)\n",
    "    temp_df = temp_df[~temp_df[target_var].isnull()]\n",
    "    \n",
    "    if len(temp_df)<7:\n",
    "        temp_df = pd.DataFrame(this_data.corr(method=\"spearman\")[np.abs(this_data.corr(method=\"spearman\"))>=0.5].iloc[0,:].copy())\n",
    "        temp_df.sort_index(inplace=True)\n",
    "        temp_df = temp_df[~temp_df[target_var].isnull()]\n",
    "        print(\"\\nReduced min_corr to 0.5 for this horizon\")\n",
    "        \n",
    "    if len(temp_df)<7:\n",
    "        temp_df = pd.DataFrame(this_data.corr(method=\"spearman\")[np.abs(this_data.corr(method=\"spearman\"))>=0.4].iloc[0,:].copy())\n",
    "        temp_df.sort_index(inplace=True)\n",
    "        temp_df = temp_df[~temp_df[target_var].isnull()]\n",
    "        print(\"\\nReduced min_corr to 0.4 for this horizon\")\n",
    "        \n",
    "    if len(temp_df)<7:\n",
    "        temp_df = pd.DataFrame(this_data.corr(method=\"spearman\")[np.abs(this_data.corr(method=\"spearman\"))>=0.3].iloc[0,:].copy())\n",
    "        temp_df.sort_index(inplace=True)\n",
    "        temp_df = temp_df[~temp_df[target_var].isnull()]\n",
    "        print(\"\\nReduced min_corr to 0.3 for this horizon\")\n",
    "        \n",
    "    if len(temp_df)<7:\n",
    "        temp_df = pd.DataFrame(this_data.corr(method=\"spearman\")[np.abs(this_data.corr(method=\"spearman\"))>=0.0].iloc[0,:].copy())\n",
    "        temp_df.sort_index(inplace=True)\n",
    "        temp_df = temp_df[~temp_df[target_var].isnull()]\n",
    "        print(\"\\nReduced min_corr to 0.0 for this horizon\")\n",
    "    \n",
    "    return temp_df\n",
    "\n",
    "def plot_corr_heatmap(this_data,target_var):\n",
    "    if len(this_data)>1:\n",
    "        sns.heatmap(this_data.sort_values(by=target_var,ascending=False))\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No features with correlation above \", min_corr)\n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "def plot_scaled_scatter(this_data, selected_col, target):\n",
    "\n",
    "    this_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    selected_data = this_scaler.fit_transform(this_data[[target, selected_col]])\n",
    "    selected_data = pd.DataFrame(selected_data,columns=[target, selected_col])\n",
    "\n",
    "    plt.figure(figsize=(16,5)) \n",
    "    plt.title('Scatter of {} (X) with {} (Y)'.format(target, selected_col))\n",
    "    plt.scatter(selected_data[target].values, selected_data[selected_col].values)\n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "def split_train_validation_true_data(target_var, this_cols_retain, this_training_and_validation_data, this_true_data, test_size):\n",
    "    '''\n",
    "    splits train and validation set\n",
    "    '''     \n",
    "    this_training_and_validation_data = this_training_and_validation_data[this_cols_retain]\n",
    "    this_true_data = this_true_data[this_cols_retain]\n",
    "\n",
    "    this_cols_retain.remove(target_var)\n",
    "    Xs = this_training_and_validation_data[this_cols_retain]\n",
    "    Ys = this_training_and_validation_data[target_var]\n",
    "\n",
    "    X_train, X_val , y_train, y_val = train_test_split(Xs, Ys, test_size=test_size, random_state=42, shuffle=True)\n",
    "    \n",
    "    return X_train, X_val , y_train, y_val, this_training_and_validation_data, this_true_data\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    '''\n",
    "    function to calculate MAPE\n",
    "    '''    \n",
    "    temp_mape = []\n",
    "    for this_true, this_pred in zip(y_true, y_pred):\n",
    "        temp_val = np.abs((this_true - this_pred) / this_true)\n",
    "        temp_mape.append(temp_val)\n",
    "\n",
    "    return np.mean(temp_mape) * 100, temp_mape\n",
    "\n",
    "\n",
    "\n",
    "def train_and_select_model(my_verbose, metric, this_training_x, this_validation_x, this_training_y, this_validation_y, target_var, this_y_scaler):\n",
    "    '''\n",
    "    Train model, evaluate on validation set\n",
    "    '''\n",
    "\n",
    "    # Compile models\n",
    "    # tune ET, RF: https://stackoverflow.com/a/22546016/6877740\n",
    "    models = []\n",
    "#     models.append(('LR', LinearRegression()))\n",
    "#     models.append(('LASSO', Lasso()))\n",
    "#     models.append(('EN', ElasticNet()))\n",
    "#     models.append(('KNN', KNeighborsRegressor()))\n",
    "#     models.append(('CART', DecisionTreeRegressor()))\n",
    "#     models.append(('SVR', SVR()))\n",
    "#     models.append(('AB', AdaBoostRegressor()))\n",
    "    models.append(('GBM', GradientBoostingRegressor(n_estimators=200,max_depth=5,min_samples_leaf=2)))\n",
    "    models.append(('RF', RandomForestRegressor(n_estimators=200,max_depth=5,min_samples_leaf=2)))\n",
    "    models.append(('ET', ExtraTreesRegressor(n_estimators=200,max_depth=5,min_samples_leaf=2)))\n",
    "    model_names = [x[0] for x in models]\n",
    "\n",
    "    list_rms = []\n",
    "    list_mapes = []\n",
    "    list_rsq = []\n",
    "    list_predictions = []\n",
    "\n",
    "    descaled_validation_actual_target = inverse_scale_target(this_y_scaler,this_validation_y.values.reshape(-1, 1),target_var)\n",
    "    descaled_validation_actual_target = descaled_validation_actual_target.values.reshape(-1,1)\n",
    "    y_true = descaled_validation_actual_target    \n",
    "\n",
    "\n",
    "    for this_model in models:\n",
    "        this_model_name = this_model[0]\n",
    "        this_regressor = this_model[1]\n",
    "\n",
    "        reg = this_regressor.fit(this_training_x.values, this_training_y.values.reshape(-1,1))\n",
    "\n",
    "        # evaluate model on validation\n",
    "        predictions = reg.predict(this_validation_x.values)\n",
    "        predictions = predictions.reshape(-1,1)\n",
    "        descaled_validation_predicted_target = inverse_scale_target(this_y_scaler,predictions,target_var)\n",
    "        descaled_validation_predicted_target = descaled_validation_predicted_target.values.reshape(-1,1)        \n",
    "\n",
    "        # compute errors        \n",
    "        y_pred = descaled_validation_predicted_target\n",
    "        list_predictions.append(y_pred)\n",
    "        rms = sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mape, apes = mean_absolute_percentage_error(y_true, y_pred)\n",
    "        rsq = r2_score(y_true, y_pred)\n",
    "\n",
    "        list_rms.append(rms)\n",
    "        list_mapes.append(mape)\n",
    "        list_rsq.append(rsq)\n",
    "\n",
    "    if my_verbose==True:\n",
    "        print(\"\\nModels trained complete\")\n",
    "\n",
    "    if metric == \"RMSE\":\n",
    "        errors_list = list_rms\n",
    "        val, idx  = min((val, idx) for (idx, val) in enumerate(list_rms))\n",
    "\n",
    "        print(\"\\nLowest validation {} of: {:.2f}\".format(metric, val))\n",
    "\n",
    "    elif metric == \"MAPE\":\n",
    "        errors_list = list_mapes\n",
    "        val, idx  = min((val, idx) for (idx, val) in enumerate(list_mapes))\n",
    "\n",
    "        print(\"\\nLowest validation {} of: {:.2f}%\".format(metric, val))\n",
    "\n",
    "    elif metric == \"RSQ\":\n",
    "        errors_list = list_rsq\n",
    "        val, idx  = max((val, idx) for (idx, val) in enumerate(list_rsq))\n",
    "\n",
    "        print(\"\\nHighest validation {} of: {:.2f}%\".format(metric, val))        \n",
    "        \n",
    "        \n",
    "    best_y_pred = list_predictions[idx]\n",
    "    best_model = models[idx]\n",
    "    best_error = val\n",
    "    best_rsq = list_rsq[idx]\n",
    "    \n",
    "    # temp_df = pd.DataFrame(best_y_pred,columns=[\"y_pred\"])\n",
    "    # temp_df[\"y_true\"] = y_true\n",
    "    # temp_df.to_csv(\"checks_v2.csv\")\n",
    "\n",
    "    return y_true, best_y_pred, best_model, best_error, best_rsq\n",
    "\n",
    "\n",
    "def plot_scatter_actuals_predicted(this_actuals, this_predicted):\n",
    "    '''\n",
    "    plot actuals vs predicted. The closer to the diagonal, the better.\n",
    "    ''' \n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    ax.scatter(this_actuals, this_predicted, c='black')\n",
    "    line = mlines.Line2D([0, 1], [0, 1], color='red')\n",
    "    transform = ax.transAxes\n",
    "    line.set_transform(transform)\n",
    "    ax.add_line(line)\n",
    "    ax.set_title(\"Actual vs Predicted\")\n",
    "    ax.set_xlabel(\"Actual\")\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "    \n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "def predict_test(this_model, this_true_data, this_y_scaler, target_var, environment):\n",
    "    '''\n",
    "    predict based on test set. Always uses last time stamp in training set.\n",
    "    '''     \n",
    "    this_model_name = this_model[0]\n",
    "    this_regressor = this_model[1]\n",
    "    \n",
    "    x_cols = [x for x in this_true_data.columns.tolist() if x != target_var]\n",
    "    X_test = this_true_data[x_cols]\n",
    "    \n",
    "    if environment == \"PRD\":\n",
    "        y_test_actual = None\n",
    "        y_test_actual_descaled = None\n",
    "\n",
    "    elif environment == \"QAS\":\n",
    "        y_test_actual = this_true_data[target_var].values.reshape(-1,1)[0]   \n",
    "\n",
    "        # descale target\n",
    "        descaled_test_actual_target = inverse_scale_target(this_y_scaler,y_test_actual.reshape(-1, 1),target_var)\n",
    "        descaled_test_actual_target = descaled_test_actual_target.values.reshape(-1,1)\n",
    "        y_test_actual_descaled = descaled_test_actual_target[0]      \n",
    "\n",
    "    # get prediction\n",
    "    reg = this_regressor\n",
    "    predictions = reg.predict(X_test.values)\n",
    "    predictions = predictions.reshape(-1,1)[0]\n",
    "    descaled_test_predicted_target = inverse_scale_target(this_y_scaler,predictions.reshape(-1, 1),target_var)\n",
    "    descaled_test_predicted_target = descaled_test_predicted_target.values.reshape(-1,1)        \n",
    "    y_pred = descaled_test_predicted_target[0]\n",
    "    \n",
    "    return y_test_actual, y_test_actual_descaled, predictions, y_pred, this_model_name\n",
    "\n",
    "\n",
    "def generate_test_results(this_test_results,this_prediction_date):\n",
    "    '''\n",
    "    generates a dataframe with test results\n",
    "    '''     \n",
    "    this_actual = this_test_results[\"Actuals - Descaled\"].values\n",
    "    this_pred = this_test_results[\"Predicted - Descaled\"].values\n",
    "    \n",
    "    this_test_results[\"APE\"] =  np.abs(this_actual - this_pred) / this_actual * 100\n",
    "    test_MAPE = this_test_results.APE.mean()\n",
    "    test_rsq = r2_score(this_actual, this_pred)\n",
    "    test_rms = sqrt(mean_squared_error(this_actual , this_pred))\n",
    "    \n",
    "    prediction_date = [this_prediction_date]*len(this_test_results)\n",
    "    \n",
    "    #print(\"MAPE: {:.2f}%, RSQ: {:.2f}%, RMSE: {:.2f}\".format(test_MAPE[0], test_rsq, test_rms))\n",
    "    print(\"MAPE: {:.2f}%\".format(test_MAPE[0]))\n",
    "    \n",
    "    return this_test_results\n",
    "\n",
    "\n",
    "def plot_test_results(this_test_results):\n",
    "    '''\n",
    "    Plots test results\n",
    "    '''     \n",
    "    plt.figure(figsize=(16,5)) \n",
    "    plt.title('SPDR Gold Shares (USD): Actuals vs Predicted')\n",
    "    plt.plot(list(range(0,len(this_test_results))), this_test_results[\"Actuals - Descaled\"].values, label = \"Actual\")\n",
    "    plt.plot(list(range(0,len(this_test_results))), this_test_results[\"Predicted - Descaled\"].values, label = \"Predicted\")\n",
    "    plt.legend()\n",
    "    plt.show()    \n",
    "\n",
    "    plt.figure(figsize=(16,5)) \n",
    "    plt.title('SPDR Gold Shares (USD): % Error (Actual vs Predicted)')\n",
    "    plt.plot(list(range(0,len(this_test_results))), this_test_results[\"APE\"].values, label = \"% Error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def inspect_issues(this_test_results, this_cached_scaled_data):\n",
    "    '''\n",
    "    qas function to pick out samples with highest MAPE\n",
    "    '''     \n",
    "    this_horizon = test_results.sort_values(by=\"APE\",ascending=False).iloc[0].Horizon\n",
    "    this_APE = test_results.sort_values(by=\"APE\",ascending=False).iloc[0].APE[0]\n",
    "    print(\"Highest APE at horizon {}, with APE of {:.2f}\".format(this_horizon, this_APE))\n",
    "    temp_df = this_cached_scaled_data.copy()\n",
    "    temp_df.reset_index(inplace=True)\n",
    "    check_cols = [x[:-8] for x in dict_features[this_horizon]] # remove \"shifted suffix\"\n",
    "    print(temp_df.iloc[-91][check_cols])\n",
    "\n",
    "    temp_df.plot(x = 'index', y=target_var, kind = 'scatter')\n",
    "    plt.axvline(x=temp_df.index.max()-(this_horizon+1))\n",
    "    plt.show()\n",
    "\n",
    "    for this_col in check_cols:\n",
    "        temp_df.plot(x = 'index', y=this_col, kind = 'scatter')\n",
    "        plt.axvline(x=temp_df.index.max()-(this_horizon+1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows:  476\n",
      "\n",
      "\n",
      "HORIZON: 1\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "11 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.58%\n",
      "\n",
      "Best model is RF with RSQ: 0.99\n",
      "\n",
      "\n",
      "HORIZON: 2\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "10 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.80%\n",
      "\n",
      "Best model is GBM with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 3\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "9 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.72%\n",
      "\n",
      "Best model is RF with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 4\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "9 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.82%\n",
      "\n",
      "Best model is ET with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 5\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "8 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.76%\n",
      "\n",
      "Best model is ET with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 6\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "8 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.86%\n",
      "\n",
      "Best model is ET with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 7\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "8 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.93%\n",
      "\n",
      "Best model is RF with RSQ: 0.95\n",
      "\n",
      "\n",
      "HORIZON: 8\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "8 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.05%\n",
      "\n",
      "Best model is ET with RSQ: 0.94\n",
      "\n",
      "\n",
      "HORIZON: 9\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "8 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.12%\n",
      "\n",
      "Best model is ET with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 10\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "8 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.02%\n",
      "\n",
      "Best model is GBM with RSQ: 0.95\n",
      "\n",
      "\n",
      "HORIZON: 11\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "9 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.96%\n",
      "\n",
      "Best model is GBM with RSQ: 0.94\n",
      "\n",
      "\n",
      "HORIZON: 12\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "8 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.99%\n",
      "\n",
      "Best model is GBM with RSQ: 0.95\n",
      "\n",
      "\n",
      "HORIZON: 13\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "8 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.06%\n",
      "\n",
      "Best model is GBM with RSQ: 0.94\n",
      "\n",
      "\n",
      "HORIZON: 14\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "8 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.04%\n",
      "\n",
      "Best model is ET with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 15\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "8 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.12%\n",
      "\n",
      "Best model is ET with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 16\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "8 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.01%\n",
      "\n",
      "Best model is ET with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 17\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "8 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.78%\n",
      "\n",
      "Best model is ET with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 18\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "7 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.91%\n",
      "\n",
      "Best model is ET with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 19\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "11 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.99%\n",
      "\n",
      "Best model is GBM with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 20\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "11 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.81%\n",
      "\n",
      "Best model is GBM with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 21\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "11 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.85%\n",
      "\n",
      "Best model is GBM with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 22\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "11 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.78%\n",
      "\n",
      "Best model is GBM with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 23\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "11 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.77%\n",
      "\n",
      "Best model is GBM with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 24\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "12 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.79%\n",
      "\n",
      "Best model is GBM with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 25\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "10 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.82%\n",
      "\n",
      "Best model is GBM with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 26\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "10 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.85%\n",
      "\n",
      "Best model is ET with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 27\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "10 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.98%\n",
      "\n",
      "Best model is GBM with RSQ: 0.95\n",
      "\n",
      "\n",
      "HORIZON: 28\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "10 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.17%\n",
      "\n",
      "Best model is ET with RSQ: 0.93\n",
      "\n",
      "\n",
      "HORIZON: 29\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "10 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.89%\n",
      "\n",
      "Best model is GBM with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 30\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "9 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.73%\n",
      "\n",
      "Best model is RF with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 31\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "7 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.96%\n",
      "\n",
      "Best model is GBM with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 32\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.3 for this horizon\n",
      "15 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.94%\n",
      "\n",
      "Best model is GBM with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 33\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.3 for this horizon\n",
      "16 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.89%\n",
      "\n",
      "Best model is GBM with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 34\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.3 for this horizon\n",
      "16 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.69%\n",
      "\n",
      "Best model is GBM with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 35\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.3 for this horizon\n",
      "15 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.70%\n",
      "\n",
      "Best model is RF with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 36\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "7 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.75%\n",
      "\n",
      "Best model is GBM with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 37\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "8 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.83%\n",
      "\n",
      "Best model is GBM with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 38\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "9 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.02%\n",
      "\n",
      "Best model is RF with RSQ: 0.94\n",
      "\n",
      "\n",
      "HORIZON: 39\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "10 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.82%\n",
      "\n",
      "Best model is GBM with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 40\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "10 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.80%\n",
      "\n",
      "Best model is GBM with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 41\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "10 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.05%\n",
      "\n",
      "Best model is ET with RSQ: 0.95\n",
      "\n",
      "\n",
      "HORIZON: 42\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "10 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.85%\n",
      "\n",
      "Best model is GBM with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 43\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "10 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.91%\n",
      "\n",
      "Best model is ET with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 44\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "13 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.68%\n",
      "\n",
      "Best model is ET with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 45\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "\n",
      "Reduced min_corr to 0.4 for this horizon\n",
      "17 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.71%\n",
      "\n",
      "Best model is ET with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 46\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "7 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.03%\n",
      "\n",
      "Best model is ET with RSQ: 0.95\n",
      "\n",
      "\n",
      "HORIZON: 47\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "8 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.93%\n",
      "\n",
      "Best model is ET with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 48\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "9 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.67%\n",
      "\n",
      "Best model is RF with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 49\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "13 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.72%\n",
      "\n",
      "Best model is ET with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 50\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "12 features used\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lowest validation MAPE of: 0.77%\n",
      "\n",
      "Best model is RF with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 51\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "14 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.82%\n",
      "\n",
      "Best model is ET with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 52\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "18 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.67%\n",
      "\n",
      "Best model is ET with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 53\n",
      "\n",
      "Reduced min_corr to 0.5 for this horizon\n",
      "18 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.76%\n",
      "\n",
      "Best model is ET with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 54\n",
      "7 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.05%\n",
      "\n",
      "Best model is GBM with RSQ: 0.95\n",
      "\n",
      "\n",
      "HORIZON: 55\n",
      "9 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.15%\n",
      "\n",
      "Best model is RF with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 56\n",
      "11 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.95%\n",
      "\n",
      "Best model is GBM with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 57\n",
      "15 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.88%\n",
      "\n",
      "Best model is ET with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 58\n",
      "15 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.80%\n",
      "\n",
      "Best model is GBM with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 59\n",
      "15 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.81%\n",
      "\n",
      "Best model is RF with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 60\n",
      "16 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.79%\n",
      "\n",
      "Best model is RF with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 61\n",
      "16 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.72%\n",
      "\n",
      "Best model is RF with RSQ: 0.99\n",
      "\n",
      "\n",
      "HORIZON: 62\n",
      "16 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.67%\n",
      "\n",
      "Best model is GBM with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 63\n",
      "16 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.67%\n",
      "\n",
      "Best model is RF with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 64\n",
      "16 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.75%\n",
      "\n",
      "Best model is RF with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 65\n",
      "17 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.66%\n",
      "\n",
      "Best model is GBM with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 66\n",
      "17 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.85%\n",
      "\n",
      "Best model is ET with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 67\n",
      "18 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.75%\n",
      "\n",
      "Best model is RF with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 68\n",
      "20 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.82%\n",
      "\n",
      "Best model is ET with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 69\n",
      "22 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.75%\n",
      "\n",
      "Best model is GBM with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 70\n",
      "22 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.76%\n",
      "\n",
      "Best model is GBM with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 71\n",
      "22 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.82%\n",
      "\n",
      "Best model is ET with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 72\n",
      "24 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.81%\n",
      "\n",
      "Best model is ET with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 73\n",
      "25 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.72%\n",
      "\n",
      "Best model is GBM with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 74\n",
      "25 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.98%\n",
      "\n",
      "Best model is GBM with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 75\n",
      "25 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.85%\n",
      "\n",
      "Best model is GBM with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 76\n",
      "24 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.77%\n",
      "\n",
      "Best model is GBM with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 77\n",
      "23 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.76%\n",
      "\n",
      "Best model is GBM with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 78\n",
      "23 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.83%\n",
      "\n",
      "Best model is GBM with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 79\n",
      "23 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.89%\n",
      "\n",
      "Best model is GBM with RSQ: 0.95\n",
      "\n",
      "\n",
      "HORIZON: 80\n",
      "24 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.13%\n",
      "\n",
      "Best model is GBM with RSQ: 0.94\n",
      "\n",
      "\n",
      "HORIZON: 81\n",
      "24 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.03%\n",
      "\n",
      "Best model is GBM with RSQ: 0.92\n",
      "\n",
      "\n",
      "HORIZON: 82\n",
      "23 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.97%\n",
      "\n",
      "Best model is ET with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 83\n",
      "23 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.97%\n",
      "\n",
      "Best model is ET with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 84\n",
      "22 features used\n",
      "\n",
      "Lowest validation MAPE of: 0.84%\n",
      "\n",
      "Best model is ET with RSQ: 0.98\n",
      "\n",
      "\n",
      "HORIZON: 85\n",
      "20 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.03%\n",
      "\n",
      "Best model is GBM with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 86\n",
      "18 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.03%\n",
      "\n",
      "Best model is ET with RSQ: 0.97\n",
      "\n",
      "\n",
      "HORIZON: 87\n",
      "18 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.14%\n",
      "\n",
      "Best model is ET with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 88\n",
      "16 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.30%\n",
      "\n",
      "Best model is ET with RSQ: 0.95\n",
      "\n",
      "\n",
      "HORIZON: 89\n",
      "15 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.22%\n",
      "\n",
      "Best model is ET with RSQ: 0.96\n",
      "\n",
      "\n",
      "HORIZON: 90\n",
      "15 features used\n",
      "\n",
      "Lowest validation MAPE of: 1.07%\n",
      "\n",
      "Best model is ET with RSQ: 0.96\n",
      "\n",
      "Completed successfully\n",
      "Total elapsed time:  0:03:11\n"
     ]
    }
   ],
   "source": [
    "starting_time = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "target_var = \"SPDR_Gold_Shares\"\n",
    "my_verbose = False\n",
    "min_horizon = 1\n",
    "max_horizon = 90\n",
    "min_corr = 0.65\n",
    "metric = \"MAPE\" # or RMSE\n",
    "environment = \"PRD\" # or \"PRD\"\n",
    "stdev_multiplier=2.5\n",
    "\n",
    "\n",
    "if not os.path.exists('..\\..\\RESULTS'):\n",
    "    os.makedirs('..\\..\\RESULTS')\n",
    "\n",
    "dict_features = {}    \n",
    "\n",
    "# load data\n",
    "# raw_data = pd.read_csv(r'..\\..\\DATA\\RAW_DATA.csv')\n",
    "raw_data = pd.read_csv(r'..\\..\\DATA\\RAW_DATA_TEST_PRD_SCRIPT.csv')\n",
    "try:\n",
    "    raw_data.Date = pd.to_datetime(raw_data.Date, format='%Y-%m-%d')\n",
    "except ValueError:\n",
    "    raw_data.Date = pd.to_datetime(raw_data.Date, format=\"%d/%m/%Y\")\n",
    "    \n",
    "raw_data = remove_missing_targets(raw_data,target_var)\n",
    "filled_data = treat_missing_feature_values_adjusted(my_verbose, raw_data)\n",
    "after_data = filled_data.iloc[2000:] # time stamp to use\n",
    "after_data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "if environment == \"PRD\":\n",
    "    # if prd, create 90 blank rows\n",
    "    for i in range(min_horizon, max_horizon+1):\n",
    "        temp_list = [None,999999]\n",
    "        temp_list_nils = [None] * (len(after_data.columns.tolist())-2) #add blank last row to serve as prediction placeholder\n",
    "        temp_list.extend(temp_list_nils)\n",
    "        after_data.loc[len(after_data)] = temp_list #using loc with index value that does not exist. Be sure to reset index first\n",
    "\n",
    "training_and_validation_data = after_data.iloc[:-90]\n",
    "true_data = after_data.iloc[-90:]\n",
    "\n",
    "# clean outliers from training & validation, using only training & validation\n",
    "detrended_data = detrend_data(training_and_validation_data,my_verbose)\n",
    "detrended_data = remove_outliers(detrended_data,stdev_multiplier,my_verbose)\n",
    "keep_dates = detrended_data.Date.tolist()\n",
    "training_and_validation_data = training_and_validation_data[training_and_validation_data.Date.isin(keep_dates)]\n",
    "\n",
    "# append training & validation, with true_data\n",
    "# (we need to ensure all are in one so that we can create the same columns for subsequent preprocessing steps)\n",
    "cleansed_data = training_and_validation_data.append(true_data)\n",
    "cleansed_data.reset_index(drop=True,inplace=True)\n",
    "print(\"Total rows: \", cleansed_data.shape[0])\n",
    "\n",
    "# engineer new features expect for target. target is highly correlated with Gold Futures\n",
    "cols_to_calculate = [x for x in cleansed_data.columns.tolist() if not x.endswith(\"SPDR_Gold_Shares\")]\n",
    "cols_to_calculate.remove(\"Date\")\n",
    "transformed_data = calculate_macd_and_spreadvssignal(my_verbose, cleansed_data, cols_to_calculate)\n",
    "transformed_data = calculate_moving_averages(my_verbose, transformed_data, cols_to_calculate)\n",
    "\n",
    "# cache features before transformation\n",
    "cached_transformed_data = transformed_data.copy()\n",
    "cached_descaled_data = cached_transformed_data\n",
    "prediction_date = cached_transformed_data.Date.max().strftime('%Y-%m-%d')\n",
    "\n",
    "# scale data\n",
    "transformed_data, this_y_scaler = scale_data(transformed_data, target_var)\n",
    "\n",
    "# cache the scaled data\n",
    "cached_scaled_data = transformed_data\n",
    "\n",
    "# cache y_values for true data\n",
    "test_results = pd.DataFrame(columns=[\"Horizon\",\"Actuals - Scaled\", \"Actuals - Descaled\", \"Predicted - Scaled\", \"Predicted - Descaled\",\"Model Name\"])\n",
    "\n",
    "for forecasting_horizon in range(min_horizon, max_horizon+1):\n",
    "    \n",
    "    print(\"\\n\\nHORIZON:\", forecasting_horizon)\n",
    "    \n",
    "    # shift data\n",
    "    shifted_data = transformed_data.copy()\n",
    "    shifted_data = shift_values(my_verbose, shifted_data, forecasting_horizon, target_var)\n",
    "    \n",
    "    # separate for feature selection, which is based on training & validation\n",
    "    training_and_validation_data = shifted_data.iloc[:-90]\n",
    "    true_data = shifted_data.iloc[-90:]\n",
    "    temp_df = pd.DataFrame(columns=shifted_data.columns.tolist())\n",
    "    # from true_value, select true value we are aiming to predict ie the horizon-th row in true_data\n",
    "    true_data = temp_df.append(true_data.iloc[forecasting_horizon-1])\n",
    "        \n",
    "    # get features\n",
    "    selected_features_df = calculate_corr(target_var,min_corr,training_and_validation_data)\n",
    "    cols_retain = list(selected_features_df.index)\n",
    "    dict_features[forecasting_horizon] = cols_retain\n",
    "    print(\"{} features used\".format(len(cols_retain)))\n",
    "    \n",
    "    if environment == \"QAS\":\n",
    "        # plot_corr_heatmap uni-variate scatter\n",
    "        plot_corr_heatmap(selected_features_df,target_var)\n",
    "        data_for_plots = training_and_validation_data[list(selected_features_df.index)]\n",
    "        selected_cols = data_for_plots.columns.tolist()\n",
    "        target = \"SPDR_Gold_Shares\"\n",
    "        selected_cols.remove(target)\n",
    "        for selected_col in selected_cols:\n",
    "            plot_scaled_scatter(data_for_plots, selected_col, target)    \n",
    "\n",
    "    if my_verbose == \"True\":\n",
    "        print(\"\\nHorizon: \", forecasting_horizon, \"\\nNo. of features: \", len(cols_retain), \"\\n\\n\")\n",
    "    \n",
    "    # split train, validation, and true data\n",
    "    X_train, X_validation , y_train, y_validation, training_and_validation_data, true_data = split_train_validation_true_data(target_var, cols_retain, training_and_validation_data, true_data, test_size=0.33)\n",
    "    \n",
    "    \n",
    "    # train models and pick best based on metric\n",
    "    y_validation_descaled, best_y_pred_descaled, best_model, best_error, best_rsq = train_and_select_model(my_verbose, metric, X_train, X_validation, y_train, y_validation, target_var, this_y_scaler)\n",
    "\n",
    "    print(\"\\nBest model is\", best_model[0], \"with {}: {:.2f}\".format(\"RSQ\", best_rsq))\n",
    "    \n",
    "    if environment == \"QAS\":\n",
    "        plot_scatter_actuals_predicted(y_validation_descaled, best_y_pred_descaled)\n",
    "        \n",
    "    # get test results\n",
    "    y_test_actual, y_test_actual_descaled, prediction_scaled, prediction_descaled, this_model_name = predict_test(best_model, true_data, this_y_scaler, target_var,environment)\n",
    "    last_row = len(test_results)\n",
    "    test_results.loc[last_row] = [forecasting_horizon,y_test_actual, y_test_actual_descaled, prediction_scaled, prediction_descaled, this_model_name]        \n",
    "    \n",
    "\n",
    "if (test_results[\"Actuals - Scaled\"].sum() == cached_scaled_data.iloc[-90:].SPDR_Gold_Shares.sum()) and (environment == \"QAS\"):\n",
    "    print(\"Successfully cached true values\")\n",
    "    \n",
    "# generate results and plot graph    \n",
    "if environment == \"QAS\":\n",
    "    test_results = generate_test_results(test_results,prediction_date)\n",
    "    plot_test_results(test_results)\n",
    "\n",
    "# output results\n",
    "todays_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "test_results.to_csv(r'..\\..\\RESULTS\\saved_forecasts_{}.csv'.format(\"90_day_sample\"),index=False)\n",
    "\n",
    "cached_descaled_data.to_csv(r'..\\..\\RESULTS\\saved_descaled_data.csv',index=False)    \n",
    "cached_scaled_data.to_csv(r'..\\..\\RESULTS\\saved_scaled_data.csv',index=False)   \n",
    "print(\"\\nCompleted successfully\")\n",
    "ending_time = datetime.datetime.now().replace(microsecond=0)\n",
    "print(\"Total elapsed time: \", ending_time-starting_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if environment == \"QAS\":\n",
    "    inspect_issues(test_results, cached_scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
